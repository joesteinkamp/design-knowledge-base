
# Product & Communication Metrics

## Usability
*Outcomes*
- Decrease task time and tool time
- Decrease cost for product training
- Increase conversion
- Increase user satisfaction

### Task Completion Rate
The percentage of users that are able to complete a desired task, such as place an order. Project/Feature specific metric. The fundamental usability metric is task completion. If users cannot complete what they came to do in a website or software, then not much else matters. While a "good" completion rate always depends on context, we've found that in over 1,100 tasks the average task completion rate is a 78%.

*Measure by:* Usability Tests; FullStory

### Task Time
The total time spent within specific task’s flow from start to finish. Pick key flows to monitor. Use the time to compare over time, compare against an expected time, and compare other users’ times. Total task duration is the de facto measure of efficiency and productivity. Record how long it takes a user to complete a task in seconds and or minutes. Start task times when users finish reading task scenarios and end the time when users have finished all actions (including reviewing).

Measure by: Usability Tests/Benchmarks; FullStory

### Error Rate
The total number of errors within a specific task’s flow. Pick key flows to monitor. Across 719 tasks of mostly consumer and business software, we found that by counting the number of slips and mistakes about two out of every three users (2/3) had an error (0.7 errors per task). Only 10% of all tasks we've observed are error-free or, in other words, to err is human.

Measure by: Usability Tests; FullStory

### Confusion Moments
The number of moments the user was uncertain what to do but was able to guess correctly, effectively not making an error. This is very hard to measure accurately.

Measure by: Usability Tests

### Task Confidence Score
At the end of each task, people are asked how confident they are. For example, low confidence but correctly completed the task a slight negative impact on the usability assessment.

Measure by: Likert Scale - “How confident are you that you completed your task correctly?” Scale: 1 (very low confidence) to 5 (very high confidence)

### Customer Effort Score
A simple measurement to track how much effort the user is putting into use the tool. After users attempt a task, have them answer the customer effort question. This metric will immediately flag a difficult task, especially when compared to a database of other tasks.

Measure by: Likert Scale – “How much effort did you personally have to put forth to complete your task?” Scale: 1 (very low effort) to 5 (very high effort)

### Learnability (Learning Curve)
Through testing that involves repetitive tasks, you can understand how many repetitions it takes for participants to learn how to successfully or efficiently accomplish a task. You can be obtain this information from metrics your remote usability testing tool has captured. Decreased time on task, clicks per task, and increased success ratios indicate improved usability through learning.

### xpected Difficulty Vs Interpreted Difficulty
Users have an expectation about how difficult a task should be based on subtle cues in the task-scenario. Asking users how difficult they expect a task to be and comparing it to actual task difficulty ratings (from the same or different users) can be useful in diagnosing problem areas.

### First Time User Success Rate (Learnability)
A sub-metric of task completion. This metric looks at only first time users of a given product or, feature and their percentage of completion. Effective at understanding how much learnability is impacting task completion (Ex. experienced users that have been trained or more likely to succeed but the more novice users can use our technology without experience the easier it is.)

Measured by: Percentage of users that complete a specific flow (Ex. Place an order) for a population of users that have never visited that task previously. (Flag new users)

### Feature Utilization (Discoverability)
The percentage of users that use features or sub-capabilities. Tracks either the perceived value of the feature or that they’re not able to discover it.

Measure by: Percentage of population that uses a specific feature; Usability Tests - where users express intent of an action but are unable to find it when it does exist

### Single Usability Metric (SUM) [source]
The SUM is the average of task metrics—completion rates, error rates, task-times, and task-difficulty ratings. As such, it is impacted by completion rates which are context-dependent and task times which fluctuate based on the complexity of the task.
Measure by: Usability Benchmarks

### Task Performance Indicator [source]
Essentially, a variant of the single usability metric. The exact equation hasn’t been released but is the SUM plus weighted scoring based on outcomes determined by confidence ratings.

### System Usability Scale [source]
A ten question questionnaire that creates a score from 0 - 100 to identify the usability of an experience. Industry studies have shown a score of 68 or above is above average.

### Drop Off Rate
Percent of people that don’t take the next step that you’d like them to, includes exit rate plus users that navigate away from the desired flow.

### Exit Rate
Percent of people that leave your product at a specific page/screen.

## Usability: Cost to Support Poor Experience
*Outcomes*
- Reduction of operation cost (ex. less tickets = less people)
- Decrease cost for product training
- Increase user satisfaction
- Increase revenue from existing customers by giving the operations team members more time to engage with customers.
- Increase market share by having clients provide good word-of-mouth experiences.

### Average Number of Support Tickets Per Customer
Track the average number of support tickets per customer. Hypothesis: Clients struggling with your service will have more tickets.

Measure by: Total number of support tickets divided by number of customers per month. 
Total Time Spent Supporting Tickets

### Amount of time spent supporting tickets. Focus on both specialists and clients. Hypothesis: an easier experience will require less support ticket time.

Need to stabilize by average for number of clients or specialist or shifts happening. 

Measure by: Survey Support Teams, Time to ticket closure

### Helpdesk Views
Track the number of views of the help desk to get help.

Measure by: Number of helpdesk views divided by number of client users

### Training Time
Length of time to onboard new customers to use our products. 

Measure by: Amount of time sales takes to onboard new customers

### Training Material Creation Time
Amount of time and resources spent to create effective training material.

Measure by: Survey instructional designer

### Number of Orders Placed in Ops Manager
The percentage of orders placed in by operations team members instead of clients.

Measure by: Percent of orders placed by admins

## Efficiency Metrics

### Task Time
The total time spent within specific task’s flow from start to finish. Pick key flows to monitor. Use the time to compare over time, compare against an expected time, and compare other users’ times. Total task duration is the de facto measure of efficiency and productivity. Record how long it takes a user to complete a task in seconds and or minutes. Start task times when users finish reading task scenarios and end the time when users have finished all actions (including reviewing).

Measure by: Usability Tests/Benchmarks, FullStory

### Number of Clicks/Taps
The minimum number of clicks/taps that are needed to complete a given task. You should not follow this too closely but good to be aware of number of steps needed. Important to consider perception of speed.

### Customer Effort Score [Duplicated in Usability]
A simple measurement to track how much effort the user is putting into use the tool. After users attempt a task, have them answer the customer effort question. This metric will immediately flag a difficult task, especially when compared to a database of other tasks.

Measure by: Likert Scale – “How much effort did you personally have to put forth to complete your task?” Scale: 1 (very low effort) to 5 (very high effort)

### Drop Off Rate [Duplicated in Usability]
Percent of people that don’t take the next step that you’d like them to, includes exit rate plus users that navigate away from the desired flow.

### Exit Rate [Duplicated in Usability]
Percent of people that leave your product at a specific page/screen.

## Desirability Metrics
*Outcomes*
- Higher retention
- Higher perceived usability
- More likely to refer new users

### NPS Score
A metric for measuring how likely a user would be to recommend the site to others. Track the score over time. Not a great metric.

### NPS Comments: Number of Negative Sentiments
Track the number of negative sentiments per month. 

### Satisfaction Rating
A customer satisfaction rating either developed in-house or using an external tool like ForeSee to monitor our customer’s satisfaction. (Private Marketplace)

Measure by: Attitudinal survey

### See Retention Metrics Below
Retention metrics are also good indicators that the users find the experience desirable.

### Referral Rates
Two ways to measure: number of referrals sent by customers and/or the number of customers that we referred.

## Performance/Reliability Metrics

### Load Time
Length of time a page or screen takes to load. Key screens to monitor: Orders, Roster Details,  Place Order.

### Number of Bug Reports
Number of bugs reported per feature.

### Server Errors
Number of service errors we receive in the logs.

### Uptime
Percentage of time our product is accessible.

### Number of Requests
Amount of data request needed to load the UI

### Request Size
Total file size of all the assets requested.

### Bandwidth
The transmission speed of a connection to the Internet. Main factor in determining how fast an experience can be loaded for a user. 

### Latency
Network speed measurement that is the time it takes between each request. Each request as a delay to make the connection - this delay is the latency. It is completely separate from bandwidth.

## Useful: Engagement Metrics

### Daily Active Users
Reflects what percentage of your user base comes back each day. This is a key engagement metric because it signals high engagement, as it only includes registered users who visit each day (thus they are returning visitors).

### Monthly Active Users
Number of users that are active each month.

### Frequency
Measures how often people return to the web site.  It is calculated by dividing the total number of visits by the total number of unique visitors. It is often used to measure loyalty. 

### Downloads to Active Users Ratio
Percentage of active users of total possible users.

### Attention Minutes
How long a user is paying attention to content such as videos.

### First Impressions
How users initially react upon seeing the site for the first time.

### Interactions
How often does the user like, comment on or share content?

### Interaction Depth
How many times does the user click when navigating through the site.

### Drop Off Rate [Duplicated in Usability & Efficiency]
Percent of people that don’t take the next step that you’d like them to, includes exit rate plus users that navigate away from the desired flow.

### Exit Rate [Duplicated in Usability & Efficiency]
Percent of people that leave your product at a specific page/screen.

## Useful: Retention Metrics


### Upgrades to the Latest App
Percentage of users on that upgrade to the latest version of the app.

### Retention Rate
Number of customers that stay.

### Churn Rate
Number of customers that leave.

### Return Visitors
Number of customers that return to the app.

### Lifetime Value
The prediction of the net profit attributed to the entire future relationship with a customer.

## Useful: Adoption Metrics
*Outcomes*
- Less reliance on top-tier employees + empower all users = more productivity


### Registered Users
Registered users is a valuable engagement metric because it includes only those people who took some step to become a user in the system. This means they made some decision to create an account, which immediately separates them from those who merely visit.

### Customers
These are people who have completed a customer transaction with you, so are very valuable people to be aware of. They often include those registered users who decided to go the extra step and give you money. 

### Number of App Downloads
Total number of app downloads.

### Number of Visits
Total number of visits.

### Rate of Users Using Advanced Features
Do we even have features that only a subset are using most often? Number of users divided by 

### Number of Impressions
Total number of visits / page loads.

### Unique Visitors
Number of unique visitors that open your product and account for impressions.

### Return Rate
Number of unique visitors that account for multiple impressions by returning to the product.


# Design Team Operations

### Measure Design Time of a Project
How long does it take to complete the design work. Might be difficult because we’re usually prescribed a length of time. Plus, risky to optimize toward faster delivery rather than quality.

### Measure Lead Time of a Project
Measure how far in advance design gets to work. Try to tie this to outcomes to determine best amount of lead time for successful projects.

### Design Backlog Velocity
Measure how quickly the Design Backlog work is being completed.

### Number of Times Design is a Blocker
